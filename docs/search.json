{
  "articles": [
    {
      "path": "about.html",
      "title": "About this site",
      "description": "Some additional details about the website",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2021-11-03T19:26:02-05:00"
    },
    {
      "path": "blog.html",
      "title": "Blog",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2021-11-03T19:26:03-05:00"
    },
    {
      "path": "index.html",
      "title": "Karis Cox",
      "author": [],
      "contents": "\n\n          \n          \n          Karis Cox\n          \n          \n          Home\n          \n          \n          About\n           \n          ▾\n          \n          \n          Resume\n          \n          \n          Blog\n          R Squared\n          ☰\n          \n          \n      \n        \n          \n            Karis Cox\n          \n          \n            \n              I am a Graduate Student at Mississippi State University with plans to graduate in May with a Master of Professional Accountancy and a minor in Data Analytics. I will then go to work at Byrne Zizzi, CPA in Tupelo, MS as a Tax Associate. I enjoy travelling, learning, and spending time with my family.\n            \n            \n              I am a Graduate Student at Mississippi State University with plans to graduate in May with a Master of Professional Accountancy and a minor in Data Analytics. I will then go to work at Byrne Zizzi, CPA in Tupelo, MS as a Tax Associate. I enjoy travelling, learning, and spending time with my family.\n            \n          \n\n          \n            \n              \n                  \n                    \n                      LinkedIn\n                    \n                  \n                \n                                \n                  \n                    \n                      GitHub\n                    \n                  \n                \n                                \n                  \n                    \n                      Email\n                    \n                  \n                \n                              \n          \n\n          \n            \n              \n                                \n                  \n                    LinkedIn\n                  \n                \n                                \n                  \n                    GitHub\n                  \n                \n                                \n                  \n                    Email\n                  \n                \n                              \n            \n          \n        \n      \n    \n\n    \n    \n    ",
      "last_modified": "2021-11-03T19:26:03-05:00"
    },
    {
      "path": "resumeforwebsite.html",
      "title": "Karis Cox",
      "author": [],
      "contents": "\n\nContents\nEducation\nRelevant Work Experience\nGraduate Teaching Assistant\nAdministrative Assistant\n\nHonors and Activities\nSkills\n\nkariscox.kc@gmail.com\n662-542-1650\n5474 Macedonia Road\nHoulka, MS 38850\nEducation\nMississippi State University. Mississippi State, MS\nExpected Graduation May 2022\nMaster of Professional Accountancy\nMinor in Data Analytics\nGPA: 4.0/4.0\nMississippi State University. Mississippi State, MS\nGraduated May 2021\nBachelor of Accountancy\nGPA: 4.0/4.0\nRelevant Work Experience\nGraduate Teaching Assistant\nJune 2021-May 2022\n\nAdkerson School of Accountancy at Mississippi State University\n• Conduct research for assigned faculty members\n• Engage in literature searches\n• Contribute to evaluation of accounting related teaching materials\n• Lead students in supplemental instruction of accounting material\nAdministrative Assistant\nSeptember 2016-March 2020\n\nByrne Zizzi CPA, Tupelo, MS\n• Maintained electronic and paper files\n• Input data into accounting database\n• Gathered information and created spreadsheets outlining client’s tax data\n• Assisted tax preparers in preparation of income tax\n• Welcomed visitors, answered phone calls, and maintained reception area\n• Coordinated and scheduled appointments accurately\nHonors and Activities\n• Phi Theta Kappa (2017-2019)\n• President’s List (2017-current)\n• Valedictorian & Leadership Scholarship Recipient (2017-2019)\n• Business Administration Division Chair Representative (2019)\n• Alpha Psi chapter of Delta Gamma Fraternity (2019-2020)\n• ICC Honor College (2017)\n• North Mississippi Big Brother/Big Sister (2017-2018)\n• Gamma Beta Phi (2019-2020)\n• Beta Alpha Psi Honor Society (2020-current)\n• Stephen D. Lee Scholar (2021)\nSkills\n• Proficient in Microsoft Word, PowerPoint and Excel\n• Proficient in Thomson Reuters Accounting Software\n• Working knowledge of Microsoft Access\n• Efficient file management\n• Strong written and oral communication skills\n\n\n\n",
      "last_modified": "2021-11-03T19:26:03-05:00"
    },
    {
      "path": "Rsquaredforwebsite.html",
      "title": "R Squared",
      "description": "Analyzing the Use of R Squared in Statistics and Research\n",
      "author": [
        {
          "name": "Karis Cox- from the class of Dr. Hunt",
          "url": {}
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nR-squared is a statistic that often accompanies regression output. It ranges in value from 0 to 1 and is usually interpreted as summarizing the percent of variation in the response that the regression model explains. So an R-squared of 0.65 might mean that the model explains about 65% of the variation in our dependent variable. Given this logic, we prefer our regression models have a high R-squared.\nIn R, we typically get R-squared by calling the summary function on a model object. Here’s a quick example using simulated data:\n\n\n# independent variable\nx <- 1:20 \n# for reproducibility\nset.seed(1) \n# dependent variable; function of x with random error\ny <- 2 + 0.5*x + rnorm(20,0,3) \n# simple linear regression\nmod <- lm(y~x)\n# request just the r-squared value\nsummary(mod)$r.squared          \n\n\n[1] 0.6026682\n\nOne way to express R-squared is as the sum of squared fitted-value deviations divided by the sum of squared original-value deviations:\n\\[\nR^{2} =  \\frac{\\sum (\\hat{y} – \\bar{\\hat{y}})^{2}}{\\sum (y – \\bar{y})^{2}}\n\\]\nWe can calculate it directly using our model object like so:\n\n\n# extract fitted (or predicted) values from model\nf <- mod$fitted.values\n# sum of squared fitted-value deviations\nmss <- sum((f - mean(f))^2)\n# sum of squared original-value deviations\ntss <- sum((y - mean(y))^2)\n# r-squared\nmss/tss                      \n\n\n[1] 0.6026682\n\n1. R-squared does not measure goodness of fit. It can be arbitrarily low when the model is completely correct. By making\\(σ^2\\) large, we drive R-squared towards 0, even when every assumption of the simple linear regression model is correct in every particular.\nWhat is \\(σ^2\\)? When we perform linear regression, we assume our model almost predicts our dependent variable. The difference between “almost” and “exact” is assumed to be a draw from a Normal distribution with mean 0 and some variance we call \\(σ^2\\).\nThis statement is easy enough to demonstrate. The way we do it here is to create a function that (1) generates data meeting the assumptions of simple linear regression (independent observations, normally distributed errors with constant variance), (2) fits a simple linear model to the data, and (3) reports the R-squared. Notice the only parameter for sake of simplicity is sigma. We then “apply” this function to a series of increasing \\(σ\\) values and plot the results.\n\n\nr2.0 <- function(sig){\n  # our predictor\n  x <- seq(1,10,length.out = 100)   \n  # our response; a function of x plus some random noise\n  y <- 2 + 1.2*x + rnorm(100,0,sd = sig) \n  # print the R-squared value\n  summary(lm(y ~ x))$r.squared          \n}\nsigmas <- seq(0.5,20,length.out = 20)\n # apply our function to a series of sigma values\nrout <- sapply(sigmas, r2.0)            \nplot(rout ~ sigmas, type=\"b\")\n\n\n\n\nR-squared tanks hard with increasing sigma, even though the model is completely correct in every respect.\nR-squared can be arbitrarily close to 1 when the model is totally wrong.\nThe point being made is that R-squared does not measure goodness of fit.\n\n\nset.seed(1)\n# our predictor is data from an exponential distribution\nx <- rexp(50,rate=0.005)\n# non-linear data generation\ny <- (x-1)^2 * runif(50, min=0.8, max=1.2) \n# clearly non-linear\nplot(x,y)             \n\n\n\n\n\n\nsummary(lm(y ~ x))$r.squared\n\n\n[1] 0.8485146\n\nIt’s very high at about 0.85, but the model is completely wrong. Using R-squared to justify the “goodness” of our model in this instance would be a mistake. Hopefully one would plot the data first and recognize that a simple linear regression in this case would be inappropriate.\n3. R-squared says nothing about prediction error, even with \\(σ^2\\) exactly the same, and no change in the coefficients. R-squared can be anywhere between 0 and 1 just by changing the range of X. We’re better off using Mean Square Error (MSE) as a measure of prediction error.\nMSE is basically the fitted y values minus the observed y values, squared, then summed, and then divided by the number of observations.\nLet’s demonstrate this statement by first generating data that meets all simple linear regression assumptions and then regressing y on x to assess both R-squared and MSE.\n\n\nx <- seq(1,10,length.out = 100)\nset.seed(1)\ny <- 2 + 1.2*x + rnorm(100,0,sd = 0.9)\nmod1 <- lm(y ~ x)\nsummary(mod1)$r.squared\n\n\n[1] 0.9383379\n\n# Mean squared error\nsum((fitted(mod1) - y)^2)/100\n\n\n[1] 0.6468052\n\nNow repeat the above code, but this time with a different range of x. Leave everything else the same:\n\n\n # new range of x\nx <- seq(1,2,length.out = 100)      \nset.seed(1)\ny <- 2 + 1.2*x + rnorm(100,0,sd = 0.9)\nmod1 <- lm(y ~ x)\nsummary(mod1)$r.squared\n\n\n[1] 0.1502448\n\n# Mean squared error\nsum((fitted(mod1) - y)^2)/100        \n\n\n[1] 0.6468052\n\nThe R-squared falls from 0.94 to 0.15 but the MSE remains the same. In other words the predictive ability is the same for both data sets, but the R-squared would lead you to believe the first example somehow had a model with more predictive power.\nR-squared can easily go down when the model assumptions are better fulfilled.\nLet’s examine this by generating data that would benefit from transformation. Notice the R code below is very much like our previous efforts but now we exponentiate our y variable.\n\n\nx <- seq(1,2,length.out = 100)\nset.seed(1)\ny <- exp(-2 - 0.09*x + rnorm(100,0,sd = 2.5))\nsummary(lm(y ~ x))$r.squared\n\n\n[1] 0.003281718\n\nplot(lm(y ~ x), which=3)\n\n\n\n\nR-squared is very low and our residuals vs. fitted plot reveals outliers and non-constant variance. A common fix for this is to log transform the data. Let’s try that and see what happens:\n\n\nplot(lm(log(y)~x),which = 3) \n\n\n\n\nThe diagnostic plot looks much better. Our assumption of constant variance appears to be met. But look at the R-squared:\n\n\nsummary(lm(log(y)~x))$r.squared \n\n\n[1] 0.0006921086\n\nIt’s even lower! This is an extreme case and it doesn’t always happen like this. In fact, a log transformation will usually produce an increase in R-squared. But as just demonstrated, assumptions that are better fulfilled don’t always lead to higher R-squared.\nIt is very common to say that R-squared is “the fraction of variance explained” by the regression. \\[Yet\\] if we regressed X on Y, we’d get exactly the same R-squared. This in itself should be enough to show that a high R-squared says nothing about explaining one variable by another.\nThis is the easiest statement to demonstrate:\n\n\nx <- seq(1,10,length.out = 100)\ny <- 2 + 1.2*x + rnorm(100,0,sd = 2)\nsummary(lm(y ~ x))$r.squared\n\n\n[1] 0.737738\n\nsummary(lm(x ~ y))$r.squared\n\n\n[1] 0.737738\n\nDoes x explain y, or does y explain x? Are we saying “explain” to dance around the word “cause”? In a simple scenario with two variables such as this, R-squared is simply the square of the correlation between x and y:\n\n\nall.equal(cor(x,y)^2, summary(lm(x ~ y))$r.squared, summary(lm(y ~ x))$r.squared)\n\n\n[1] TRUE\n\nLet’s recap:\nR-squared does not measure goodness of fit.\nR-squared does not measure predictive error.\nR-squared does not necessarily increase when assumptions are better satisfied.\nR-squared does not measure how one variable explains another.\n\n\n\n",
      "last_modified": "2021-11-03T19:26:04-05:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
