---
title: "Machine Learning-Clustering"
description: |
  An example of a machine learning model for dimensionality reduction
author:
  - name: Karis Cox- from the class of Dr. Hunt
date: "`r Sys.Date()`"
output: distill::distill_article
---

```{r setup, include=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(caret)
trainIndex <- createDataPartition(iris$Species, p = .6, list = FALSE, times = 1)

irisTrain <- iris[ trainIndex,]
irisTest  <- iris[-trainIndex,]

preProcValues <- preProcess(irisTrain, method = c("center", "scale"))
trainTransformed <- predict(preProcValues, irisTrain)

preProcValues <- preProcess(irisTest, method = c("center", "scale"))
testTransformed <- predict(preProcValues, irisTest)
```

# Clustering Models

Clustering is an unsupervised learning method used in machine learning. Unsupervised means there is not a target in mind. Rather, we are simply noticing the patterns in the data. In the following examples, I use K-means clustering, which makes centers in the data and assigns all data points to the closest center. The data points are then classified based on their center. I use 2, 3, and 4 cluster centers to see how well the data is centered around these cluster centers.

## Two Cluster Centers

This is the full data set based on 2 clusters in a table form.

```{r}
TwoClusters<-kmeans(trainTransformed[,-5],centers=2)
Clusterdata<-trainTransformed
Clusterdata$Cluster<-as.factor(TwoClusters$cluster)
knitr::kable(Clusterdata)%>%
  kableExtra::kable_styling("striped")%>%
  kableExtra::scroll_box(width = "100%",height="300px")
```

Here are a few graphic representations showing how the data is split when 2 clusters are used. As you can see, it splits one of the species pretty accurately. However, the other 2 do not split well when only two clusters are used.

```{r}
ggplot(data=Clusterdata,mapping = aes(x=Sepal.Width,y=Petal.Width,color=Cluster))+geom_point(alpha=0.5)
```

```{r}
ggplot(data=Clusterdata,mapping = aes(x=Sepal.Width,y=Petal.Width,color=Cluster))+geom_point(alpha=0.5)+facet_wrap(~Species)
```

```{r}
ggplot(data=Clusterdata,mapping = aes(x=Sepal.Width,y=Petal.Width,color=Species))+geom_point(alpha=0.5) + 
  geom_point(data=as.data.frame(TwoClusters$centers), aes(color="Cluster center"), size=5) + theme(legend.title = element_blank())+ggtitle("Iris Cluster Demonstration")
```

## Three Cluster Centers

This is a table of the data when 3 data clusters are used.

```{r}
ThreeClusters<-kmeans(trainTransformed[,-5],centers=3)
Clusterdata<-trainTransformed
Clusterdata$Cluster<-as.factor(ThreeClusters$cluster)
knitr::kable(Clusterdata)%>%
  kableExtra::kable_styling("striped")%>%
  kableExtra::scroll_box(width = "100%",height="300px")
```

These graphs show how the data is split when 3 clusters are used rather than 2. As you can see, using 3 centers allows for a more accurate split of the data.

```{r}
ggplot(data=Clusterdata,mapping = aes(x=Sepal.Width,y=Petal.Width,color=Cluster))+geom_point(alpha=0.5)
```

```{r}
ggplot(data=Clusterdata,mapping = aes(x=Sepal.Width,y=Petal.Width,color=Cluster))+geom_point(alpha=0.5)+facet_wrap(~Species)
```

```{r}
ggplot(data=Clusterdata,mapping = aes(x=Sepal.Width,y=Petal.Width,color=Species))+geom_point(alpha=0.5) + 
  geom_point(data=as.data.frame(ThreeClusters$centers), aes(color="Cluster center"), size=5) + theme(legend.title = element_blank())+ggtitle("Iris Cluster Demonstration")
```

## Four Cluster Centers

And lastly, here is a table form of the data when 4 clusters are used.

```{r}
FourClusters<-kmeans(trainTransformed[,-5],centers=4)
Clusterdata<-trainTransformed
Clusterdata$Cluster<-as.factor(FourClusters$cluster)
knitr::kable(Clusterdata)%>%
  kableExtra::kable_styling("striped")%>%
  kableExtra::scroll_box(width = "100%",height="300px")
```

Here are some visual representations of what the data would look like when split between 4 cluster centers. Because there are only 3 species, 4 cluster centers does not prove to be extra beneficial.

```{r}
ggplot(data=Clusterdata,mapping = aes(x=Sepal.Width,y=Petal.Width,color=Cluster))+geom_point(alpha=0.5)
```

```{r}
ggplot(data=Clusterdata,mapping = aes(x=Sepal.Width,y=Petal.Width,color=Cluster))+geom_point(alpha=0.5)+facet_wrap(~Species)
```

```{r}
ggplot(data=Clusterdata,mapping = aes(x=Sepal.Width,y=Petal.Width,color=Species))+geom_point(alpha=0.5) + 
  geom_point(data=as.data.frame(FourClusters$centers), aes(color="Cluster center"), size=5) + theme(legend.title = element_blank())+ggtitle("Iris Cluster Demonstration")
```

# Clustering in the Real World

I have demonstrated how clustering can be used in machine learning, but what are some ways that this can be used in the real world? There are many scenarios when clustering could be beneficial. It is often used in marketing to identify households or individuals who are similar to each other to better understand how they need to market certain products to them. The same goes for streaming services. Businesses like Netflix and Hulu use clustering to identify subscribers who tend to watch similar shows or movies so they can provide better recommendations to them. It can also be used in the insurance industry. Insurance providers can identify households who are in the same cluster to see how similar they are based on how they use their insurance throughout the year. The insurance company can then set monthly premiums for a whole group based on their similar actions. Clustering also proves to be beneficial to the profession of accounting. Specifically, it can be used in auditing to identify outliers. This allows auditors to examine those numbers more closely and see what they are so high/low.
